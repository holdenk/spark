== Physical Plan ==
* HashAggregate (25)
+- Exchange (24)
   +- * HashAggregate (23)
      +- * Project (22)
         +- * SortMergeJoin FullOuter (21)
            :- * Sort (10)
            :  +- Exchange (9)
            :     +- * HashAggregate (8)
            :        +- Exchange (7)
            :           +- * HashAggregate (6)
            :              +- * Project (5)
            :                 +- * BroadcastHashJoin Inner BuildRight (4)
            :                    :- * ColumnarToRow (2)
            :                    :  +- Scan parquet spark_catalog.default.store_sales (1)
            :                    +- ReusedExchange (3)
            +- * Sort (20)
               +- Exchange (19)
                  +- * HashAggregate (18)
                     +- Exchange (17)
                        +- * HashAggregate (16)
                           +- * Project (15)
                              +- * BroadcastHashJoin Inner BuildRight (14)
                                 :- * ColumnarToRow (12)
                                 :  +- Scan parquet spark_catalog.default.catalog_sales (11)
                                 +- ReusedExchange (13)


(1) Scan parquet spark_catalog.default.store_sales
Output [3]: [ss_item_sk#1, ss_customer_sk#2, ss_sold_date_sk#3]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(ss_sold_date_sk#3), dynamicpruningexpression(ss_sold_date_sk#3 IN dynamicpruning#4)]
ReadSchema: struct<ss_item_sk:int,ss_customer_sk:int>

(2) ColumnarToRow [codegen id : 2]
Input [3]: [ss_item_sk#1, ss_customer_sk#2, ss_sold_date_sk#3]

(3) ReusedExchange [Reuses operator id: 30]
Output [1]: [d_date_sk#5]

(4) BroadcastHashJoin [codegen id : 2]
Left keys [1]: [ss_sold_date_sk#3]
Right keys [1]: [d_date_sk#5]
Join type: Inner
Join condition: None

(5) Project [codegen id : 2]
Output [2]: [ss_item_sk#1, ss_customer_sk#2]
Input [4]: [ss_item_sk#1, ss_customer_sk#2, ss_sold_date_sk#3, d_date_sk#5]

(6) HashAggregate [codegen id : 2]
Input [2]: [ss_item_sk#1, ss_customer_sk#2]
Keys [2]: [ss_customer_sk#2, ss_item_sk#1]
Functions: []
Aggregate Attributes: []
Results [2]: [ss_customer_sk#2, ss_item_sk#1]

(7) Exchange
Input [2]: [ss_customer_sk#2, ss_item_sk#1]
Arguments: hashpartitioning(ss_customer_sk#2, ss_item_sk#1, 5), ENSURE_REQUIREMENTS, [plan_id=1]

(8) HashAggregate [codegen id : 3]
Input [2]: [ss_customer_sk#2, ss_item_sk#1]
Keys [2]: [ss_customer_sk#2, ss_item_sk#1]
Functions: []
Aggregate Attributes: []
Results [2]: [ss_customer_sk#2 AS customer_sk#6, ss_item_sk#1 AS item_sk#7]

(9) Exchange
Input [2]: [customer_sk#6, item_sk#7]
Arguments: rangepartitioning(customer_sk#6 ASC NULLS FIRST, item_sk#7 ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [plan_id=2]

(10) Sort [codegen id : 4]
Input [2]: [customer_sk#6, item_sk#7]
Arguments: [customer_sk#6 ASC NULLS FIRST, item_sk#7 ASC NULLS FIRST], false, 0

(11) Scan parquet spark_catalog.default.catalog_sales
Output [3]: [cs_bill_customer_sk#8, cs_item_sk#9, cs_sold_date_sk#10]
Batched: true
Location: InMemoryFileIndex []
PartitionFilters: [isnotnull(cs_sold_date_sk#10), dynamicpruningexpression(cs_sold_date_sk#10 IN dynamicpruning#4)]
ReadSchema: struct<cs_bill_customer_sk:int,cs_item_sk:int>

(12) ColumnarToRow [codegen id : 6]
Input [3]: [cs_bill_customer_sk#8, cs_item_sk#9, cs_sold_date_sk#10]

(13) ReusedExchange [Reuses operator id: 30]
Output [1]: [d_date_sk#11]

(14) BroadcastHashJoin [codegen id : 6]
Left keys [1]: [cs_sold_date_sk#10]
Right keys [1]: [d_date_sk#11]
Join type: Inner
Join condition: None

(15) Project [codegen id : 6]
Output [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Input [4]: [cs_bill_customer_sk#8, cs_item_sk#9, cs_sold_date_sk#10, d_date_sk#11]

(16) HashAggregate [codegen id : 6]
Input [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Keys [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Functions: []
Aggregate Attributes: []
Results [2]: [cs_bill_customer_sk#8, cs_item_sk#9]

(17) Exchange
Input [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Arguments: hashpartitioning(cs_bill_customer_sk#8, cs_item_sk#9, 5), ENSURE_REQUIREMENTS, [plan_id=3]

(18) HashAggregate [codegen id : 7]
Input [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Keys [2]: [cs_bill_customer_sk#8, cs_item_sk#9]
Functions: []
Aggregate Attributes: []
Results [2]: [cs_bill_customer_sk#8 AS customer_sk#12, cs_item_sk#9 AS item_sk#13]

(19) Exchange
Input [2]: [customer_sk#12, item_sk#13]
Arguments: rangepartitioning(customer_sk#12 ASC NULLS FIRST, item_sk#13 ASC NULLS FIRST, 5), ENSURE_REQUIREMENTS, [plan_id=4]

(20) Sort [codegen id : 8]
Input [2]: [customer_sk#12, item_sk#13]
Arguments: [customer_sk#12 ASC NULLS FIRST, item_sk#13 ASC NULLS FIRST], false, 0

(21) SortMergeJoin [codegen id : 9]
Left keys [2]: [customer_sk#6, item_sk#7]
Right keys [2]: [customer_sk#12, item_sk#13]
Join type: FullOuter
Join condition: None

(22) Project [codegen id : 9]
Output [2]: [customer_sk#6, customer_sk#12]
Input [4]: [customer_sk#6, item_sk#7, customer_sk#12, item_sk#13]

(23) HashAggregate [codegen id : 9]
Input [2]: [customer_sk#6, customer_sk#12]
Keys: []
Functions [3]: [partial_sum(CASE WHEN (isnotnull(customer_sk#6) AND isnull(customer_sk#12)) THEN 1 ELSE 0 END), partial_sum(CASE WHEN (isnull(customer_sk#6) AND isnotnull(customer_sk#12)) THEN 1 ELSE 0 END), partial_sum(CASE WHEN (isnotnull(customer_sk#6) AND isnotnull(customer_sk#12)) THEN 1 ELSE 0 END)]
Aggregate Attributes [3]: [sum#14, sum#15, sum#16]
Results [3]: [sum#17, sum#18, sum#19]

(24) Exchange
Input [3]: [sum#17, sum#18, sum#19]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=5]

(25) HashAggregate [codegen id : 10]
Input [3]: [sum#17, sum#18, sum#19]
Keys: []
Functions [3]: [sum(CASE WHEN (isnotnull(customer_sk#6) AND isnull(customer_sk#12)) THEN 1 ELSE 0 END), sum(CASE WHEN (isnull(customer_sk#6) AND isnotnull(customer_sk#12)) THEN 1 ELSE 0 END), sum(CASE WHEN (isnotnull(customer_sk#6) AND isnotnull(customer_sk#12)) THEN 1 ELSE 0 END)]
Aggregate Attributes [3]: [sum(CASE WHEN (isnotnull(customer_sk#6) AND isnull(customer_sk#12)) THEN 1 ELSE 0 END)#20, sum(CASE WHEN (isnull(customer_sk#6) AND isnotnull(customer_sk#12)) THEN 1 ELSE 0 END)#21, sum(CASE WHEN (isnotnull(customer_sk#6) AND isnotnull(customer_sk#12)) THEN 1 ELSE 0 END)#22]
Results [3]: [sum(CASE WHEN (isnotnull(customer_sk#6) AND isnull(customer_sk#12)) THEN 1 ELSE 0 END)#20 AS store_only#23, sum(CASE WHEN (isnull(customer_sk#6) AND isnotnull(customer_sk#12)) THEN 1 ELSE 0 END)#21 AS catalog_only#24, sum(CASE WHEN (isnotnull(customer_sk#6) AND isnotnull(customer_sk#12)) THEN 1 ELSE 0 END)#22 AS store_and_catalog#25]

===== Subqueries =====

Subquery:1 Hosting operator id = 1 Hosting Expression = ss_sold_date_sk#3 IN dynamicpruning#4
BroadcastExchange (30)
+- * Project (29)
   +- * Filter (28)
      +- * ColumnarToRow (27)
         +- Scan parquet spark_catalog.default.date_dim (26)


(26) Scan parquet spark_catalog.default.date_dim
Output [2]: [d_date_sk#5, d_month_seq#26]
Batched: true
Location [not included in comparison]/{warehouse_dir}/date_dim]
PushedFilters: [IsNotNull(d_month_seq), GreaterThanOrEqual(d_month_seq,1200), LessThanOrEqual(d_month_seq,1211), IsNotNull(d_date_sk)]
ReadSchema: struct<d_date_sk:int,d_month_seq:int>

(27) ColumnarToRow [codegen id : 1]
Input [2]: [d_date_sk#5, d_month_seq#26]

(28) Filter [codegen id : 1]
Input [2]: [d_date_sk#5, d_month_seq#26]
Condition : (((isnotnull(d_month_seq#26) AND (d_month_seq#26 >= 1200)) AND (d_month_seq#26 <= 1211)) AND isnotnull(d_date_sk#5))

(29) Project [codegen id : 1]
Output [1]: [d_date_sk#5]
Input [2]: [d_date_sk#5, d_month_seq#26]

(30) BroadcastExchange
Input [1]: [d_date_sk#5]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=6]

Subquery:2 Hosting operator id = 11 Hosting Expression = cs_sold_date_sk#10 IN dynamicpruning#4


